{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nu4K9f0KKcH"
   },
   "source": [
    "Please make sure your code runs and the graphs and figures are displayed in your notebook before submitting (use %matplotlib inline).\n",
    "\n",
    "Additionally, upload any images you plan to incorporate in your notebook as attachments so we can see them in case the uploaded images don't appear properly on our end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbT9XKQFKMtO"
   },
   "source": [
    "# 1. Model Complexity and Bias/Variance Trade-off (20 points)\n",
    "\n",
    "i) Load the data given in `all_data_q5.npy` using `numpy.load()` function. This dataset contains the train and test datasets in `(x_train, y_train)` and `(x_test, y_test)` respectively.\n",
    "\n",
    "Now, fit the polynomial models of degrees 1, 5, and 10 on the training data, and print out the mean squared error for train and test datasets for all the models. Essentially, we are trying to fit linear models of this form: $\\hat f(x) = \\beta_0 + \\beta_1x + \\beta_1 x^2 + ... + \\beta_px^p$, where $p$ is the degree of the polynomial. (5 points)\n",
    "\n",
    "ii) Visualise the trained models by making predictions on evenly spaced numbers on x-axis in a fixed range, for eg. you can generate x's by calling `x_all = np.linspace(0, 1, 75).reshape(-1,1)` and call predict on x_all.\n",
    "\n",
    "In the same figure, add the following plots:\n",
    "\n",
    "a) Train data plot : y_train vs x_train\n",
    "\n",
    "b) Test data plot : y_test vs x_test\n",
    "\n",
    "All the plots must clearly labeled. (5 points)\n",
    "\n",
    "\n",
    "\n",
    "**Tips**: you can use `np.vander(np.squeeze(x_train), deg+1)` to generate the `deg`-degree polynomial vector of `x_train`. For example, `np.vander(np.squeeze(x_train), 3)` gives you the second-degree polynomial of `x_train` and you can call `np.vander` inside the fit method of linear regression.\n",
    "\n",
    "\n",
    "Make use of the starter code we have provided, and fill the `plot_curves` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2BVXQcOKvGC"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "data_load = np.load('./all_data_q5.npy', allow_pickle=True)\n",
    "x_train = data_load.item().get(\"x_train\")\n",
    "y_train = data_load.item().get(\"y_train\")\n",
    "x_test = data_load.item().get(\"x_test\")\n",
    "y_test = data_load.item().get(\"y_test\")\n",
    "\n",
    "lrp = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IVpHLIFK1ZP"
   },
   "outputs": [],
   "source": [
    "def plot_curves(x_train, y_train, x_test, y_test):\n",
    "  # Fit polynomial models of degrees 1, 5, 10 to the training data.\n",
    "  # Print out the mean squared error (on both train and test sets) for all the models.\n",
    "  # Plot the data (y_train vs x_train and y_test vs x_test), the fitted models (predictions on x_all by different models vs x_all), and the predictions on the test set (predictions on x_test by different models vs x_test).\n",
    "\n",
    "  # YOUR CODE COMES HERE\n",
    "  print(\"IMPLEMENT ME!\")\n",
    "  return [0, 0, 0], [0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XETfHhmqK3Vv"
   },
   "source": [
    "iii) Which model gives the best performance (measured by MSE)? Explain in terms of the bias-variance tradeoff. (5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJXaZ14hLN4z"
   },
   "source": [
    "iv) Analyse how the training data size affects bias and variance of the models. For this, run the analysis in (i) with the given (x_train, y_train) and (x_test, y_test), but now randomly select 20%, 40%, 60%, 80% and all 100% of the data from the train dataset to use for training the model but keep the test dataset same in each case.\n",
    "\n",
    "Now, for each of the three models corresponding to the degrees 1, 5 and 10, plot $log(MSE)$ on train dataset vs the size of the training data and again $log(MSE)$ on test dataset vs the size of the training data. State the trends you see as you change the size of the training data on each of the models and explain why you see them. All plots must be in a single figure and labelled correctly. Study the effects of the training data size on the bias and variance (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU5mvRD_JHY_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
